{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, itertools, pickle, random, glob, imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load_album(datapath, labelpath, imgsize):\n",
    "    \"\"\"\n",
    "    datapath 'jpeg' file of album image\n",
    "    labelpath 'png' file of spectrogram \n",
    "    \"\"\"\n",
    "    datalist = os.listdir(datapath)\n",
    "    datasize = len(datalist)\n",
    "    #print(datasize)\n",
    "    \n",
    "    data_ = np.zeros((datasize, imgsize, imgsize, 3))\n",
    "    label_ = np.zeros((datasize, imgsize, imgsize, 3))\n",
    "    outputlist = []\n",
    "    nonelist = []\n",
    "    for i, fname in enumerate(datalist):\n",
    "        if glob.glob(os.path.join(labelpath, fname[:-4] + '*')):\n",
    "        #f os.path.isfile(os.path.join(labelpath, fname[:-4] + 'png')):\n",
    "            #print('there is file')\n",
    "            img_d = Image.open(os.path.join(datapath, fname)\n",
    "                              ).resize((imgsize, imgsize))\n",
    "            img_l = Image.open(os.path.join(labelpath, fname[:-4]+'png')\n",
    "                              ).convert('RGB').resize((imgsize, imgsize))\n",
    "            data_[i] = np.asarray(img_d)\n",
    "            label_[i] = np.asarray(img_l)\n",
    "            img_d.close()\n",
    "            img_l.close()\n",
    "            #print(fname[:-4])\n",
    "            outputlist.append(fname[:-5])\n",
    "            \n",
    "        else:\n",
    "            nonelist.append(i)\n",
    "            #print(i, 'is empty')\n",
    "            \n",
    "    data = np.delete(data_, nonelist, 0)\n",
    "    label = np.delete(label_, nonelist, 0)\n",
    "    return data, label, outputlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load albumdata\n",
    "\n",
    "img_size = 64\n",
    "\n",
    "datapath = os.path.join('WGAN-GP-TensorFlow', 'datasets', 'album')\n",
    "audiopath = os.path.join('audio','spec256')\n",
    "\n",
    "samples, labels, datalist= dataset_load_album(\n",
    "    datapath, audiopath, img_size)\n",
    "\n",
    "samples = samples/255\n",
    "labels = labels/255\n",
    "\n",
    "input_dim = img_size * img_size * 3\n",
    "num_sample = samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_samples.npy', samples)\n",
    "np.save('x_labels.npy', labels)\n",
    "np.save('x_datalist.npy', datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantionalAutoencoder(object):\n",
    "\n",
    "    def __init__(self, input_dim =input_dim, learning_rate=1e-4, batch_size=64, n_z=16):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_z = n_z\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        self.build()\n",
    "\n",
    "        self.sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,log_device_placement=True))\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # Build the netowrk and the loss functions\n",
    "    def build(self):\n",
    "        \n",
    "        self.x = tf.placeholder(\n",
    "            name='x', dtype=tf.float32, shape=[None, self.input_dim])\n",
    "\n",
    "        # Encode\n",
    "        # x -> z_mean, z_sigma -> z\n",
    "        f1 = fc(self.x, 2048, scope='enc_fc1', activation_fn=tf.nn.relu)\n",
    "        f2 = fc(f1, 1024, scope='enc_fc2', activation_fn=tf.nn.relu)\n",
    "        f3 = fc(f2, 512, scope='enc_fc3', activation_fn=tf.nn.relu)\n",
    "        f4 = fc(f3, 256, scope='enc_fc4', activation_fn=tf.nn.relu)\n",
    "        f5 = fc(f4, 128, scope='enc_fc5', activation_fn=tf.nn.relu)\n",
    "        f6 = fc(f5, 64, scope='enc_fc6', activation_fn=tf.nn.relu)\n",
    "        self.z_mu = fc(f6, self.n_z, scope='enc_fc7_mu', \n",
    "                       activation_fn=None)\n",
    "        self.z_log_sigma_sq = fc(f6, self.n_z, scope='enc_fc7_sigma', \n",
    "                                 activation_fn=None)\n",
    "        eps = tf.random_normal(\n",
    "            shape=tf.shape(self.z_log_sigma_sq),\n",
    "            mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.z_log_sigma_sq)) * eps\n",
    "\n",
    "        # Decode\n",
    "        # z -> x_hat\n",
    "        g1 = fc(self.z, 64, scope='dec_fc1', activation_fn=tf.nn.relu)\n",
    "        g2 = fc(g1, 128, scope='dec_fc2', activation_fn=tf.nn.relu)\n",
    "        g3 = fc(g2, 256, scope='dec_fc3', activation_fn=tf.nn.relu)\n",
    "        g4 = fc(g3, 1024, scope='dec_fc4', activation_fn=tf.nn.relu)\n",
    "        g5 = fc(g4, 1024, scope='dec_fc5', activation_fn=tf.nn.relu)\n",
    "        g6 = fc(g5, 2048, scope='dec_fc6', activation_fn=tf.nn.relu)\n",
    "        self.x_hat = fc(g6, self.input_dim, scope='dec_fc7', \n",
    "                        activation_fn=tf.sigmoid)\n",
    "\n",
    "        # Loss\n",
    "        # Reconstruction loss\n",
    "        # Minimize the cross-entropy loss\n",
    "        # H(x, x_hat) = -\\Sigma x*log(x_hat) + (1-x)*log(1-x_hat)\n",
    "        epsilon = 1e-10\n",
    "        recon_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(epsilon+self.x_hat) + \n",
    "            (1-self.x) * tf.log(epsilon+1-self.x_hat), \n",
    "            axis=1\n",
    "        )\n",
    "        self.recon_loss = tf.reduce_mean(recon_loss)\n",
    "\n",
    "        # Latent loss\n",
    "        # KL divergence: measure the difference between two distributions\n",
    "        # Here we measure the divergence between \n",
    "        # the latent distribution and N(0, 1)\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mu) - \n",
    "            tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "\n",
    "        self.total_loss = self.recon_loss + self.latent_loss\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.total_loss)\n",
    "        \n",
    "        self.losses = {\n",
    "            'recon_loss': self.recon_loss,\n",
    "            'latent_loss': self.latent_loss,\n",
    "            'total_loss': self.total_loss,\n",
    "        }        \n",
    "        return\n",
    "\n",
    "    # Execute the forward and the backward pass\n",
    "    def run_single_step(self, x):\n",
    "        _, losses = self.sess.run(\n",
    "            [self.train_op, self.losses],\n",
    "            feed_dict={self.x: x}\n",
    "        )\n",
    "        return losses\n",
    "\n",
    "    # x -> x_hat\n",
    "    def reconstructor(self, x, sess):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.x: x})\n",
    "        return x_hat\n",
    "\n",
    "    # z -> x\n",
    "    def generator(self, z):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.z: z})\n",
    "        return x_hat\n",
    "    \n",
    "    # x -> z\n",
    "    def transformer(self, x):\n",
    "        z = self.sess.run(self.z, feed_dict={self.x: x})\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_album(model_object, sample, input_dim =input_dim, learning_rate=1e-4, \n",
    "            batch_size=16, num_epoch=5, n_z=16, log_step=5,\n",
    "                 num_sample = num_sample):\n",
    "    model = model_object(\n",
    "        learning_rate=learning_rate, batch_size=batch_size, n_z=n_z,\n",
    "    input_dim =input_dim)\n",
    "    \n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()\n",
    "        for iter in range(num_sample // batch_size):\n",
    "            step += 1\n",
    "            # Get a batch\n",
    "            batch = sample[iter * batch_size : (iter + 1) * batch_size]\n",
    "            # Execute the forward and backward pass \n",
    "            # Report computed losses\n",
    "            #print('batch',batch)\n",
    "            losses = model.run_single_step(batch)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if epoch % log_step == 0:\n",
    "            log_str = '[Epoch {}] '.format(epoch)\n",
    "            for k, v in losses.items():\n",
    "                log_str += '{}: {:.3f}  '.format(k, v)\n",
    "            log_str += '({:.3f} sec/epoch)'.format(end_time - start_time)\n",
    "            print(log_str)\n",
    "            \n",
    "    print('Done!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-53ba1e90f61f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_2d_vae = trainer_album(VariantionalAutoencoder, samples.reshape(-1,input_dim), \n\u001b[1;32m      6\u001b[0m                              \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                              input_dim =input_dim, num_sample=num_sample)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ba73df19e9e2>\u001b[0m in \u001b[0;36mtrainer_album\u001b[0;34m(model_object, sample, input_dim, learning_rate, batch_size, num_epoch, n_z, log_step, num_sample)\u001b[0m\n\u001b[1;32m      4\u001b[0m     model = model_object(\n\u001b[1;32m      5\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_z\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     input_dim =input_dim)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-15d650f82573>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, learning_rate, batch_size, n_z)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.sess = tf.InteractiveSession(config=tf.ConfigProto(\n\u001b[0;32m---> 13\u001b[0;31m         allow_soft_placement=True,log_device_placement=True))\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs599pro/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_pruned_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_session_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs599pro/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "#with tf.device('/cpu:0'):\n",
    "with tf.device('/gpu:0'):\n",
    "    model_2d_vae = trainer_album(VariantionalAutoencoder, samples.reshape(-1,input_dim), \n",
    "                             num_epoch=200, batch_size=64, n_z=2, \n",
    "                             input_dim =input_dim, num_sample=num_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model: generation\n",
    "# Sample noise vectors from N(0, 1)\n",
    "z = np.random.normal(size=[model_2d_vae.batch_size, model_2d_vae.n_z])\n",
    "x_generated = model_2d_vae.generator(z)\n",
    "\n",
    "n = np.sqrt(model_2d_vae.batch_size).astype(np.int32)\n",
    "I_generated = np.empty((img_size*n, img_size*n, 3))\n",
    "#print(x_generated.shape)\n",
    "for i in range(n):\n",
    "    #print(i)\n",
    "    I_generated[i*img_size:(i+1)*img_size, i*img_size:(i+1)*img_size, :\n",
    "               ] = x_generated[i].reshape(img_size, img_size, 3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and save latent z\n",
    "z_transform = model_2d_vae.transformer(samples.reshape(-1,img_size*img_size*3))\n",
    "np.save('z_tras.npy', z_transform)\n",
    "np.save('z_list.npy', datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_val_load = np.load('z_tras.npy')\n",
    "z_list_load = list(np.load('z_list.npy'))\n",
    "print(type(z_list_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(z_list_load))\n",
    "print(z_val_load.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
