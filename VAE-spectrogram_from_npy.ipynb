{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os, time, itertools, pickle, random, glob, imageio\n",
    "from PIL import Image\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load albumdata\n",
    "\n",
    "img_size = 64\n",
    "num_epoch =500\n",
    "batch_size = 64\n",
    "n_z = 64\n",
    "z_val_name = 'z64_tras.npy'\n",
    "z_list_name = 'z64_list.npy'\n",
    "\n",
    "\n",
    "samples = np.load('vae/x_labels_64.npy')\n",
    "samples = (1 - samples)\n",
    "#samples = np.load('vae/x_samples_64.npy')\n",
    "#labels = np.load('vae/x_labels_64.npy')\n",
    "\n",
    "input_dim = img_size * img_size * 3\n",
    "num_sample = samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantionalAutoencoder(object):\n",
    "\n",
    "    def __init__(self, input_dim =input_dim, learning_rate=1e-4, batch_size=64, n_z=16):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_z = n_z\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        self.build()\n",
    "\n",
    "        self.sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,log_device_placement=True))\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # Build the netowrk and the loss functions\n",
    "    def build(self):\n",
    "        \n",
    "        self.x = tf.placeholder(\n",
    "            name='x', dtype=tf.float32, shape=[None, self.input_dim])\n",
    "\n",
    "        # Encode\n",
    "        # x -> z_mean, z_sigma -> z\n",
    "        f1 = fc(self.x, 2048, scope='enc_fc1', activation_fn=tf.nn.relu)\n",
    "        print(f1)\n",
    "        f2 = fc(f1, 1024, scope='enc_fc2', activation_fn=tf.nn.relu)\n",
    "        f3 = fc(f2, 512, scope='enc_fc3', activation_fn=tf.nn.relu)\n",
    "        f4 = fc(f3, 256, scope='enc_fc4', activation_fn=tf.nn.relu)\n",
    "        f5 = fc(f4, 128, scope='enc_fc5', activation_fn=tf.nn.relu)\n",
    "        f6 = fc(f5, 64, scope='enc_fc6', activation_fn=tf.nn.relu)\n",
    "        self.z_mu = fc(f6, self.n_z, scope='enc_fc7_mu', \n",
    "                       activation_fn=None)\n",
    "        self.z_log_sigma_sq = fc(f6, self.n_z, scope='enc_fc7_sigma', \n",
    "                                 activation_fn=None)\n",
    "        eps = tf.random_normal(\n",
    "            shape=tf.shape(self.z_log_sigma_sq),\n",
    "            mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.z_log_sigma_sq)) * eps\n",
    "\n",
    "        # Decode\n",
    "        # z -> x_hat\n",
    "        g1 = fc(self.z, 64, scope='dec_fc1', activation_fn=tf.nn.relu)\n",
    "        g2 = fc(g1, 128, scope='dec_fc2', activation_fn=tf.nn.relu)\n",
    "        g3 = fc(g2, 256, scope='dec_fc3', activation_fn=tf.nn.relu)\n",
    "        g4 = fc(g3, 1024, scope='dec_fc4', activation_fn=tf.nn.relu)\n",
    "        g5 = fc(g4, 1024, scope='dec_fc5', activation_fn=tf.nn.relu)\n",
    "        g6 = fc(g5, 2048, scope='dec_fc6', activation_fn=tf.nn.relu)\n",
    "        self.x_hat = fc(g6, self.input_dim, scope='dec_fc7', \n",
    "                        activation_fn=tf.sigmoid)\n",
    "\n",
    "        # Loss\n",
    "        # Reconstruction loss\n",
    "        # Minimize the cross-entropy loss\n",
    "        # H(x, x_hat) = -\\Sigma x*log(x_hat) + (1-x)*log(1-x_hat)\n",
    "        epsilon = 1e-10\n",
    "        recon_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(epsilon+self.x_hat) + \n",
    "            (1-self.x) * tf.log(epsilon+1-self.x_hat), \n",
    "            axis=1\n",
    "        )\n",
    "        self.recon_loss = tf.reduce_mean(recon_loss)\n",
    "\n",
    "        # Latent loss\n",
    "        # KL divergence: measure the difference between two distributions\n",
    "        # Here we measure the divergence between \n",
    "        # the latent distribution and N(0, 1)\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mu) - \n",
    "            tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "\n",
    "        self.total_loss = self.recon_loss + self.latent_loss\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.total_loss)\n",
    "        \n",
    "        self.losses = {\n",
    "            'recon_loss': self.recon_loss,\n",
    "            'latent_loss': self.latent_loss,\n",
    "            'total_loss': self.total_loss,\n",
    "        }        \n",
    "        return\n",
    "\n",
    "    # Execute the forward and the backward pass\n",
    "    def run_single_step(self, x):\n",
    "        _, losses = self.sess.run(\n",
    "            [self.train_op, self.losses],\n",
    "            feed_dict={self.x: x}\n",
    "        )\n",
    "        return losses\n",
    "\n",
    "    # x -> x_hat\n",
    "    def reconstructor(self, x):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.x: x})\n",
    "        return x_hat\n",
    "\n",
    "    # z -> x\n",
    "    def generator(self, z):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.z: z})\n",
    "        return x_hat\n",
    "    \n",
    "    # x -> z\n",
    "    def transformer(self, x):\n",
    "        z = self.sess.run(self.z, feed_dict={self.x: x})\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_album(model_object, sample, input_dim =input_dim, learning_rate=1e-4, \n",
    "            batch_size=16, num_epoch=5, n_z=16, log_step=5,\n",
    "                 num_sample = num_sample):\n",
    "    model = model_object(\n",
    "        learning_rate=learning_rate, batch_size=batch_size, n_z=n_z,\n",
    "    input_dim =input_dim)\n",
    "    \n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()\n",
    "        for iter in range(num_sample // batch_size):\n",
    "            step += 1\n",
    "            # Get a batch\n",
    "            batch = sample[iter * batch_size : (iter + 1) * batch_size]\n",
    "            # Execute the forward and backward pass \n",
    "            # Report computed losses\n",
    "            #print('batch',batch)\n",
    "            losses = model.run_single_step(batch)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if epoch % log_step == 0:\n",
    "            log_str = '[Epoch {}] '.format(epoch)\n",
    "            for k, v in losses.items():\n",
    "                log_str += '{}: {:.3f}  '.format(k, v)\n",
    "            log_str += '({:.3f} sec/epoch)'.format(end_time - start_time)\n",
    "            print(log_str)\n",
    "            \n",
    "    print('Done!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"enc_fc1/Relu:0\", shape=(?, 2048), dtype=float32)\n",
      "[Epoch 0] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.216 sec/epoch)\n",
      "[Epoch 5] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.052 sec/epoch)\n",
      "[Epoch 10] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.080 sec/epoch)\n",
      "[Epoch 15] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.070 sec/epoch)\n",
      "[Epoch 20] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.056 sec/epoch)\n",
      "[Epoch 25] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.093 sec/epoch)\n",
      "[Epoch 30] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.919 sec/epoch)\n",
      "[Epoch 35] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.985 sec/epoch)\n",
      "[Epoch 40] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.006 sec/epoch)\n",
      "[Epoch 45] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.075 sec/epoch)\n",
      "[Epoch 50] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.046 sec/epoch)\n",
      "[Epoch 55] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.035 sec/epoch)\n",
      "[Epoch 60] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.043 sec/epoch)\n",
      "[Epoch 65] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.088 sec/epoch)\n",
      "[Epoch 70] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.035 sec/epoch)\n",
      "[Epoch 75] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.030 sec/epoch)\n",
      "[Epoch 80] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.952 sec/epoch)\n",
      "[Epoch 85] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.920 sec/epoch)\n",
      "[Epoch 90] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.014 sec/epoch)\n",
      "[Epoch 95] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.899 sec/epoch)\n",
      "[Epoch 100] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.885 sec/epoch)\n",
      "[Epoch 105] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.981 sec/epoch)\n",
      "[Epoch 110] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.980 sec/epoch)\n",
      "[Epoch 115] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.892 sec/epoch)\n",
      "[Epoch 120] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.897 sec/epoch)\n",
      "[Epoch 125] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.945 sec/epoch)\n",
      "[Epoch 130] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.925 sec/epoch)\n",
      "[Epoch 135] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.030 sec/epoch)\n",
      "[Epoch 140] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.894 sec/epoch)\n",
      "[Epoch 145] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.912 sec/epoch)\n",
      "[Epoch 150] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.912 sec/epoch)\n",
      "[Epoch 155] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.961 sec/epoch)\n",
      "[Epoch 160] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.971 sec/epoch)\n",
      "[Epoch 165] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.042 sec/epoch)\n",
      "[Epoch 170] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.988 sec/epoch)\n",
      "[Epoch 175] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.976 sec/epoch)\n",
      "[Epoch 180] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.991 sec/epoch)\n",
      "[Epoch 185] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.000 sec/epoch)\n",
      "[Epoch 190] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.023 sec/epoch)\n",
      "[Epoch 195] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.032 sec/epoch)\n",
      "[Epoch 200] recon_loss: nan  latent_loss: nan  total_loss: nan  (19.009 sec/epoch)\n",
      "[Epoch 205] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.937 sec/epoch)\n",
      "[Epoch 210] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.928 sec/epoch)\n",
      "[Epoch 215] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.927 sec/epoch)\n",
      "[Epoch 220] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.928 sec/epoch)\n",
      "[Epoch 225] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.942 sec/epoch)\n",
      "[Epoch 230] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.926 sec/epoch)\n",
      "[Epoch 235] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.924 sec/epoch)\n",
      "[Epoch 240] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.918 sec/epoch)\n",
      "[Epoch 245] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.901 sec/epoch)\n",
      "[Epoch 250] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.889 sec/epoch)\n",
      "[Epoch 255] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.869 sec/epoch)\n",
      "[Epoch 260] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.921 sec/epoch)\n",
      "[Epoch 265] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.881 sec/epoch)\n",
      "[Epoch 270] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.853 sec/epoch)\n",
      "[Epoch 275] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.844 sec/epoch)\n",
      "[Epoch 280] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.857 sec/epoch)\n",
      "[Epoch 285] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.846 sec/epoch)\n",
      "[Epoch 290] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.851 sec/epoch)\n",
      "[Epoch 295] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.846 sec/epoch)\n",
      "[Epoch 300] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.935 sec/epoch)\n",
      "[Epoch 305] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.912 sec/epoch)\n",
      "[Epoch 310] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.857 sec/epoch)\n",
      "[Epoch 315] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.852 sec/epoch)\n",
      "[Epoch 320] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.847 sec/epoch)\n",
      "[Epoch 325] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.959 sec/epoch)\n",
      "[Epoch 330] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.904 sec/epoch)\n",
      "[Epoch 335] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.855 sec/epoch)\n",
      "[Epoch 340] recon_loss: nan  latent_loss: nan  total_loss: nan  (18.940 sec/epoch)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model_2d_vae = trainer_album(VariantionalAutoencoder, samples.reshape(-1,input_dim), \n",
    "                             num_epoch=num_epoch, batch_size=batch_size, n_z=n_z, \n",
    "                             input_dim =input_dim, num_sample=num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model: generation\n",
    "# Sample noise vectors from N(0, 1)\n",
    "z = np.random.normal(size=[model_2d_vae.batch_size, model_2d_vae.n_z])\n",
    "x_generated = model_2d_vae.generator(z)\n",
    "\n",
    "n = np.sqrt(model_2d_vae.batch_size).astype(np.int32)\n",
    "I_generated = np.empty((img_size*n, img_size*n, 3))\n",
    "#print(x_generated.shape)\n",
    "for i in range(n):\n",
    "    #print(i)\n",
    "    I_generated[i*img_size:(i+1)*img_size, i*img_size:(i+1)*img_size, :\n",
    "               ] = x_generated[i].reshape(img_size, img_size, 3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and save latent z\n",
    "z_transform = model_2d_vae.transformer(samples.reshape(-1,img_size*img_size*3))\n",
    "np.save(z_val_name, z_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = model_2d_vae.reconstructor(samples[1].reshape(-1, 64*64*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_hat.reshape(64,64,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
