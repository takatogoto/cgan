{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cDCGAN for album\n",
    "condition vae of spectrogram 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, itertools, pickle, random, glob, imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_grid(Xs, padding):\n",
    "    N, H, W, C = Xs.shape\n",
    "    grid_size = int(math.ceil(math.sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size + 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size + 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = padding, H + padding\n",
    "    for y in range(grid_size):\n",
    "        x0, x1 = padding, W + padding\n",
    "        for x in range(grid_size):\n",
    "            if next_idx < N:\n",
    "                img = Xs[next_idx]\n",
    "                grid[y0:y1, x0:x1] = img\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    return grid\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_random_seed(seed)\n",
    "\n",
    "def conv2d(input, kernel_size, stride, num_filter, name = 'conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d(input, W, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def conv2d_transpose(input, kernel_size, stride, num_filter, name = 'conv2d_transpose'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, num_filter, input.get_shape()[3]]\n",
    "        output_shape = tf.stack([tf.shape(input)[0], tf.shape(input)[1] * 2, tf.shape(input)[2] * 2, num_filter])\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d_transpose(input, W, output_shape, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def fc(input, num_output, name = 'fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        num_input = input.get_shape()[1]\n",
    "        W = tf.get_variable('w', [num_input, num_output], tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [num_output], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def batch_norm(input, is_training):\n",
    "    out = tf.contrib.layers.batch_norm(input, decay = 0.99, center = True, scale = True,\n",
    "                                       is_training = is_training, updates_collections = None)\n",
    "    return out\n",
    "\n",
    "def leaky_relu(input, alpha = 0.2):\n",
    "    return tf.maximum(alpha * input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import sys\n",
    "    if sys.version_info.major == 2:\n",
    "        import cPickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "        return dict['data'], dict['labels']\n",
    "    else:\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict[b'data'], dict[b'labels']\n",
    "\n",
    "def load_train_data():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(5):\n",
    "        X_, Y_ = unpickle('data/cifar-10-batches-py/data_batch_%d' % (i + 1))\n",
    "        X.append(X_)\n",
    "        Y.extend(Y_)\n",
    "    X = np.concatenate(X)\n",
    "    X = X.reshape((X.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    return X, Y\n",
    "\n",
    "def load_test_data():\n",
    "    X_, Y = unpickle('data/cifar-10-batches-py/test_batch')\n",
    "    X = X_.reshape((X_.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    return X, Y\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def dataset_load_album(datapath, labelpath, imgsize):\n",
    "    \"\"\"\n",
    "    datapath 'jpeg' file of album image\n",
    "    labelpath 'png' file of spectrogram \n",
    "    \"\"\"\n",
    "    datalist = os.listdir(datapath)\n",
    "    datasize = len(datalist)\n",
    "    \n",
    "    data_ = np.zeros((datasize, imgsize, imgsize, 3))\n",
    "    label_ = np.zeros((datasize, imgsize, imgsize, 3))\n",
    "    nonelist = []\n",
    "    for i, fname in enumerate(datalist):\n",
    "        if glob.glob(os.path.join(labelpath, fname[:-4] + '*')):\n",
    "        #f os.path.isfile(os.path.join(labelpath, fname[:-4] + 'png')):\n",
    "            #print('there is file')\n",
    "            img_d = Image.open(os.path.join(datapath, fname)\n",
    "                              ).resize((imgsize, imgsize))\n",
    "            img_l = Image.open(os.path.join(labelpath, fname[:-4]+'png')\n",
    "                              ).convert('RGB').resize((imgsize, imgsize))\n",
    "            data_[i] = np.asarray(img_d)\n",
    "            label_[i] = np.asarray(img_l)\n",
    "            \n",
    "        else:\n",
    "            nonelist.append(i)\n",
    "            \n",
    "    data = np.delete(data_, nonelist, 0)\n",
    "    label = np.delete(label_, nonelist, 0)\n",
    "    return data, label\n",
    "\n",
    "def dataset_split(data, fold):\n",
    "    size = data.shape[0]\n",
    "    id_all = np.random.choice(size, size, replace=False)\n",
    "    split = size//fold\n",
    "    return data[split:], data[:split]\n",
    "    \n",
    "    \n",
    "\n",
    "# Load albumdata\n",
    "samples = np.load('vae/x_samples_64.npy')\n",
    "labels = np.load('vae/x_labels_64.npy')\n",
    "\n",
    "\n",
    "train_samples, test_samples = dataset_split(samples, 20)\n",
    "train_labels, test_labels = dataset_split(labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 1000\n",
    "        self.batch_size = 32 #32 => 64\n",
    "        self.log_step = 500 # 50 => 500\n",
    "        self.visualize_step = 200\n",
    "        self.code_size = 64\n",
    "        self.learning_rate = 1e-4\n",
    "        self.vis_learning_rate = 1e-2\n",
    "        self.recon_steps = 100\n",
    "        self.actmax_steps = 100\n",
    "        self.img_size = 64\n",
    "        \n",
    "        self._dis_called = False\n",
    "        self._gen_called = False\n",
    "\n",
    "        self.tracked_noise = np.random.normal(0, 1, [64, self.code_size])\n",
    "        self.tracked_label = np.delete(np.transpose(np.tile(np.eye(10), 7)), np.s_[-6:], 0)\n",
    "\n",
    "        self.real_input = tf.placeholder(tf.float32,\n",
    "                                         [None, self.img_size, self.img_size, 3])\n",
    "        self.real_label = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.fake_label = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.condition_label = tf.placeholder(tf.float32, [None, self.img_size, self.img_size, 3])\n",
    "        self.noise = tf.placeholder(tf.float32, [None, self.code_size])\n",
    "        \n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.recon_sample = tf.placeholder(tf.float32, [1, self.img_size, self.img_size, 3])\n",
    "        self.actmax_label = tf.placeholder(tf.float32, [1, 1])\n",
    "        \n",
    "        with tf.variable_scope('actmax'):\n",
    "            self.actmax_code = tf.get_variable('actmax_code', [1, self.code_size],\n",
    "                                               initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        self._init_ops()\n",
    "\n",
    "    def _discriminator(self, input, y_fill):\n",
    "        # We have multiple instances of the discriminator in the same computation graph,\n",
    "        # so set variable sharing if this is not the first invocation of this function.\n",
    "        \n",
    "        # input:(?, self.img_size=64, self.img_size=64, 3)\n",
    "        # y_fill: (?, 64) vae label \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('dis', reuse = self._dis_called):\n",
    "            self._dis_called = True\n",
    "            \n",
    "            #print('dis', dis_cat.get_shape())\n",
    "            \n",
    "            dis_conv1 = conv2d(dis_cat, 4, 2, 32, 'conv1') # => (?, 32, 32, 32)\n",
    "            dis_lrelu1 = leaky_relu(dis_conv1)\n",
    "            dis_conv2 = conv2d(dis_lrelu1, 4, 2, 64, 'conv2') # => (?, 16, 16, 64)\n",
    "            dis_batchnorm2 = batch_norm(dis_conv2, self.is_train)\n",
    "            dis_lrelu2 = leaky_relu(dis_batchnorm2)\n",
    "            dis_conv3 = conv2d(dis_lrelu2, 4, 2, 128, 'conv3') # => (?, 8, 8, 128)\n",
    "            dis_batchnorm3 = batch_norm(dis_conv3, self.is_train)\n",
    "            dis_lrelu3 = leaky_relu(dis_batchnorm3)\n",
    "            dis_reshape3 = tf.reshape(dis_lrelu3, [-1, 8 * 8 * 128])\n",
    "            dis_fc4 = fc(dis_reshape3, self.code_size, 'fc4')\n",
    "            dis_cat = tf.concat([dis_fc4, y_fill], 1)\n",
    "            dis_fc5 = fc(dis_cat , 1, 'fc5')\n",
    "            return dis_fc4\n",
    "\n",
    "    def _generator(self, input, y_fill):\n",
    "        # input (?, self.code_size=64)\n",
    "        # c_label (?, 32, 32, 3)\n",
    "\n",
    "        with tf.variable_scope('gen', reuse = self._gen_called):\n",
    "            self._gen_called = True\n",
    "            \n",
    "            gen_cat = tf.concat([input, y_fill], 1)\n",
    "            #print('gen', gen_cat.get_shape())\n",
    "            \n",
    "            gen_fc1 = fc(gen_cat, 8 * 8 * 128, 'fc1')\n",
    "            gen_reshape1 = tf.reshape(gen_fc1, [-1, 8, 8, 128])  # => (?, 8, 8, 128)\n",
    "            gen_batchnorm1 = batch_norm(gen_reshape1, self.is_train)\n",
    "            gen_lrelu1 = leaky_relu(gen_batchnorm1)\n",
    "            gen_conv2 = conv2d_transpose(gen_lrelu1, 4, 2, 64, 'conv2') # => (?, 16, 16, 64)\n",
    "            gen_batchnorm2 = batch_norm(gen_conv2, self.is_train)\n",
    "            gen_lrelu2 = leaky_relu(gen_batchnorm2)\n",
    "            gen_conv3 = conv2d_transpose(gen_lrelu2, 4, 2, 32, 'conv3') # => (?, 32, 32, 32)\n",
    "            gen_batchnorm3 = batch_norm(gen_conv3, self.is_train)\n",
    "            gen_lrelu3 = leaky_relu(gen_batchnorm3)\n",
    "            gen_conv4 = conv2d_transpose(gen_lrelu3, 4, 2, 3, 'conv4') # => (?, 64, 64, 3)\n",
    "            gen_sigmoid4 = tf.sigmoid(gen_conv4)\n",
    "            return gen_sigmoid4\n",
    "        \n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def _reconstruction_loss(self, generated, target):\n",
    "        loss = tf.nn.l2_loss(generated - target)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def _loss(self, labels, logits):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def _reconstruction_loss(self, generated, target):\n",
    "        loss = tf.nn.l2_loss(generated - target)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    # Define operations\n",
    "    def _init_ops(self):\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-1: complete the definition of these operations                        #\n",
    "        ################################################################################\n",
    "        # reshape\n",
    "        #y_label_ = tf.reshape(self.condition_label, [-1, 1, 1, 10])\n",
    "        #bsize = tf.shape(self.condition_label)[0]\n",
    "        #y_fill_ = y_label_ * tf.ones([bsize, self.img_size, self.img_size, 10])\n",
    "        \n",
    "        # self.fake_samples_op = None\n",
    "        self.fake_samples_op = self._generator(self.noise, self.condition_label)\n",
    "        \n",
    "        self.dis_loss_op = self._loss(self.real_label, \n",
    "                                      self._discriminator(self.real_input, \n",
    "                                                          self.condition_label)\n",
    "                                      ) + self._loss(self.fake_label,\n",
    "            self._discriminator(self.fake_samples_op, self.condition_label))\n",
    "        \n",
    "        # self.gen_loss_op = None\n",
    "        self.gen_loss_op = self._loss(self.real_label,\n",
    "                                      self._discriminator(self.fake_samples_op, \n",
    "                                                          self.condition_label))\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-1: fix the definition of these operations                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        # dis_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        # self.dis_train_op = dis_optimizer.minimize(self.dis_loss_op)\n",
    "        dis_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                     \"dis\")\n",
    "        dis_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.dis_train_op = dis_optimizer.minimize(self.dis_loss_op,\n",
    "                                                   var_list=dis_train_vars)\n",
    "        \n",
    "        # gen_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        # self.gen_train_op = gen_optimizer.minimize(self.gen_loss_op)\n",
    "        gen_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                     \"gen\")\n",
    "        gen_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.gen_train_op = gen_optimizer.minimize(self.gen_loss_op,\n",
    "                                                  var_list=gen_train_vars)\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-4: check the definition of these operations                           #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.actmax_sample_op = self._generator(self.actmax_code, self.condition_label)\n",
    "        actmax_dis = self._discriminator(self.actmax_sample_op, self.condition_label)\n",
    "        self.actmax_loss_op = self._loss(self.actmax_label, actmax_dis)\n",
    "\n",
    "        actmax_optimizer = tf.train.AdamOptimizer(self.vis_learning_rate)\n",
    "        self.actmax_op = actmax_optimizer.minimize(self.actmax_loss_op, var_list = [self.actmax_code])\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-4: complete the definition of these operations                        #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        \n",
    "        # self.recon_loss_op = None\n",
    "        self.recon_loss_op = self._reconstruction_loss(self.actmax_sample_op, self.recon_sample)\n",
    "        \n",
    "        # recon_optimizer = tf.train.AdamOptimizer(self.vis_learning_rate)\n",
    "        recon_optimizer = tf.train.AdamOptimizer(self.vis_learning_rate)\n",
    "        \n",
    "        # self.reconstruct_op = recon_optimizer.minimize(self.recon_loss_op)\n",
    "        self.reconstruct_op = recon_optimizer.minimize(self.recon_loss_op)\n",
    "        \n",
    "        \n",
    "        ################################################################################\n",
    "        #                               END OF YOUR CODE                               #\n",
    "        ################################################################################\n",
    "\n",
    "    # Training function\n",
    "    def train(self, sess, train_samples, test_labels):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        num_train = train_samples.shape[0]\n",
    "        step = 0\n",
    "        \n",
    "        # smooth the loss curve so that it does not fluctuate too much\n",
    "        smooth_factor = 0.95\n",
    "        plot_dis_s = 0\n",
    "        plot_gen_s = 0\n",
    "        plot_ws = 0\n",
    "        \n",
    "        dis_losses = []\n",
    "        gen_losses = []\n",
    "        max_steps = int(self.num_epoch * (num_train // self.batch_size))\n",
    "        print('Start training ...')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epoch):\n",
    "            epoch_start_time = time.time()\n",
    "            for i in range(num_train // self.batch_size):\n",
    "                step += 1\n",
    "\n",
    "                batch_samples = train_samples[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                batch_labels = train_labels[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                #noise = np.random.normal(0, 1, [batch_labels.shape[0], self.code_size])\n",
    "                noise = np.random.normal(0, 1, [self.batch_size, self.code_size])\n",
    "                zeros = np.zeros([self.batch_size, 1])\n",
    "                ones = np.ones([self.batch_size, 1])\n",
    "        \n",
    "                ################################################################################\n",
    "                # Prob 2-1: complete the feed dictionary                                       #\n",
    "                ################################################################################\n",
    "                \n",
    "                # dis_feed_dict = {}\n",
    "                dis_feed_dict = {self.real_input:batch_samples, self.real_label:zeros,\n",
    "                                self.fake_label:ones, self.noise:noise, self.is_train:True,\n",
    "                                self.condition_label:batch_labels}\n",
    "                \n",
    "                ################################################################################\n",
    "                #                               END OF YOUR CODE                               #\n",
    "                ################################################################################\n",
    "                #print('batch', batch_labels.shape, 'noise', noise.shape)\n",
    "\n",
    "                _, dis_loss = sess.run([self.dis_train_op, self.dis_loss_op], feed_dict = dis_feed_dict)\n",
    "        \n",
    "                ################################################################################\n",
    "                # Prob 2-1: complete the feed dictionary                                       #\n",
    "                ################################################################################\n",
    "                \n",
    "                # gen_feed_dict = {}\n",
    "                gen_feed_dict = {self.noise:noise, self.real_label:zeros,\n",
    "                                 self.fake_label:ones, self.is_train:True,\n",
    "                                 self.condition_label:batch_labels}\n",
    "                \n",
    "                ################################################################################\n",
    "                #                               END OF YOUR CODE                               #\n",
    "                ################################################################################\n",
    "\n",
    "                _, gen_loss = sess.run([self.gen_train_op, self.gen_loss_op], feed_dict = gen_feed_dict)\n",
    "\n",
    "                plot_dis_s = plot_dis_s * smooth_factor + dis_loss * (1 - smooth_factor)\n",
    "                plot_gen_s = plot_gen_s * smooth_factor + gen_loss * (1 - smooth_factor)\n",
    "                plot_ws = plot_ws * smooth_factor + (1 - smooth_factor)\n",
    "                dis_losses.append(plot_dis_s / plot_ws)\n",
    "                gen_losses.append(plot_gen_s / plot_ws)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('Iteration {0}/{1}: dis loss = {2:.4f}, gen loss = {3:.4f}'.format(step, max_steps, dis_loss, gen_loss))\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            print('Epoch {0} time is {1:.2f} [s]'.format(epoch, per_epoch_ptime))\n",
    "            \n",
    "            \n",
    "            if epoch % 5 ==0:\n",
    "                \n",
    "                fig = plt.figure(figsize = (8, 8))   \n",
    "                ax1 = plt.subplot(111)\n",
    "                ax1.imshow(viz_grid(self.generate(self.tracked_noise, test_labels), 1))\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(dis_losses)\n",
    "                plt.title('discriminator loss')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('loss')\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(gen_losses)\n",
    "                plt.title('generator loss')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('loss')\n",
    "                plt.show()\n",
    "        print('... Done!')\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        print('total time is {0:.2f} [s]'.format(total_ptime))\n",
    "\n",
    "    # Find the reconstruction of one input sample\n",
    "    def reconstruct_one_sample(self, sample):\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-4: initialize self.actmax_code                                        #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        \n",
    "        # actmax_init_val = None\n",
    "        # sample <class 'numpy.ndarray'> (1, 32, 32, 3)\n",
    "        # sample_tf = tf.convert_to_tensor(sample, dtype = tf.float32)\n",
    "        actmax_init_val = np.random.normal(0, 1, [1, self.code_size])\n",
    "        \n",
    "        self.condition_label = np.eye(10)\n",
    "        ################################################################################\n",
    "        #                               END OF YOUR CODE                               #\n",
    "        ################################################################################\n",
    "        \n",
    "        sess.run(self.actmax_code.assign(actmax_init_val))\n",
    "        last_reconstruction = None\n",
    "        last_loss = None\n",
    "        for i in range(self.recon_steps):\n",
    "        \n",
    "            ################################################################################\n",
    "            # Prob 2-4: complete the feed dictionary                                       #\n",
    "            # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "            ################################################################################   \n",
    "            confeed = np.zeros((1,10))\n",
    "            confeed[i%10] = 1\n",
    "            \n",
    "            # recon_feed_dict = {}\n",
    "            recon_feed_dict = {self.recon_sample: sample, self.actmax_label: np.ones([1, 1]),\n",
    "                               self.is_train: True, self.condition_label:confeed }\n",
    "            \n",
    "            ################################################################################\n",
    "            #                               END OF YOUR CODE                               #\n",
    "            ################################################################################\n",
    "            \n",
    "            run_ops = [self.recon_loss_op, self.reconstruct_op, self.actmax_sample_op]\n",
    "            last_loss, _, last_reconstruction = sess.run(run_ops, feed_dict = recon_feed_dict)\n",
    "        return last_loss, last_reconstruction\n",
    "\n",
    "    # Find the reconstruction of a batch of samples\n",
    "    def reconstruct(self, samples):\n",
    "        reconstructions = np.zeros(samples.shape)\n",
    "        total_loss = 0\n",
    "        for i in range(samples.shape[0]):\n",
    "            loss, reconstructions[i:i+1] = self.reconstruct_one_sample(samples[i:i+1])\n",
    "            total_loss += loss\n",
    "        return total_loss / samples.shape[0], reconstructions\n",
    "\n",
    "    # Generates a single sample from input code\n",
    "    def generate_one_sample(self, code, label):\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-1: complete the feed dictionary                                       #\n",
    "        ################################################################################\n",
    "        label_feed = label.reshape(1, self.img_size, self.img_size, 3)\n",
    "        # gen_vis_feed_dict = {}\n",
    "        gen_vis_feed_dict = {self.noise:code, self.is_train:False,\n",
    "                             self.condition_label:label_feed\n",
    "                            }\n",
    "        \n",
    "        ################################################################################\n",
    "        #                               END OF YOUR CODE                               #\n",
    "        ################################################################################\n",
    "        \n",
    "        generated = sess.run(self.fake_samples_op, feed_dict = gen_vis_feed_dict)\n",
    "        return generated\n",
    "\n",
    "    # Generates samples from input batch of codes\n",
    "    def generate(self, codes, test_labels):\n",
    "        generated = np.zeros((codes.shape[0], 32, 32, 3))\n",
    "        labelsize = test_labels.shape[0]\n",
    "        for i in range(codes.shape[0]):\n",
    "            idx = np.random.randint(labelsize)\n",
    "            generated[i:i+1] = self.generate_one_sample(codes[i:i+1], test_labels[idx])\n",
    "        return generated\n",
    "\n",
    "    # Perform activation maximization on one initial code\n",
    "    def actmax_one_sample(self, initial_code):\n",
    "        \n",
    "        ################################################################################\n",
    "        # Prob 2-4: check this function                                                #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        \n",
    "        actmax_init_val = tf.convert_to_tensor(initial_code, dtype = tf.float32)\n",
    "        sess.run(self.actmax_code.assign(actmax_init_val))\n",
    "        for i in range(self.actmax_steps):\n",
    "            actmax_feed_dict = {\n",
    "                self.actmax_label: np.ones([1, 1]),\n",
    "                self.is_train: False\n",
    "            }\n",
    "            _, last_actmax = sess.run([self.actmax_op, self.actmax_sample_op], feed_dict = actmax_feed_dict)\n",
    "        return last_actmax\n",
    "\n",
    "    # Perform activation maximization on a batch of different initial codes\n",
    "    def actmax(self, initial_codes):\n",
    "        actmax_results = np.zeros((initial_codes.shape[0], 32, 32, 3))\n",
    "        for i in range(initial_codes.shape[0]):\n",
    "            actmax_results[i:i+1] = self.actmax_one_sample(initial_codes[i:i+1])\n",
    "        return actmax_results.clip(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
    "    test_images = sess.run(G_z, {z: fixed_z_, y_label: fixed_y_, isTrain: False})\n",
    "\n",
    "    size_figure_grid = 10\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for k in range(10*10):\n",
    "        i = k // 10\n",
    "        j = k % 10\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(np.reshape(test_images[k], (img_size, img_size)), cmap='gray')\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 4 for 'gen/concat' (op: 'ConcatV2') with input shapes: [?,64], [?,64,64,3], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 4 for 'gen/concat' (op: 'ConcatV2') with input shapes: [?,64], [?,64,64,3], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-060b0676f648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#with tf.device('/cpu:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-392cbc96a54d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                initializer = tf.constant_initializer(0.0))\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-392cbc96a54d>\u001b[0m in \u001b[0;36m_init_ops\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# self.fake_samples_op = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_samples_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         self.dis_loss_op = self._loss(self.real_label, \n",
      "\u001b[0;32m<ipython-input-4-392cbc96a54d>\u001b[0m in \u001b[0;36m_generator\u001b[0;34m(self, input, y_fill)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mgen_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m#print('gen', gen_cat.get_shape())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 1033\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   1034\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 4 for 'gen/concat' (op: 'ConcatV2') with input shapes: [?,64], [?,64,64,3], []."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "mdel_path = 'model/dcgan_alubm_gpu.ckpt'\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,log_device_placement=True)) as sess:\n",
    "    #with tf.device('/cpu:0'):\n",
    "    with tf.device('/gpu:0'):\n",
    "        dcgan = DCGAN()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # restore\n",
    "        isrestore = False\n",
    "        metapath = mdel_path +'.meta'\n",
    "        if os.path.isfile(metapath):\n",
    "            dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "            gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'gen')\n",
    "            saver = tf.train.Saver(dis_var_list + gen_var_list)\n",
    "            saver.restore(sess, mdel_path)\n",
    "            #print('restore')\n",
    "            isrestore = True\n",
    "        \n",
    "        # train\n",
    "        dcgan.train(sess, train_samples, train_labels)\n",
    "        \n",
    "        #save\n",
    "        if not isrestore:\n",
    "            dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "            gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'gen')\n",
    "            saver = tf.train.Saver(dis_var_list + gen_var_list)\n",
    "        saver.save(sess, mdel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
